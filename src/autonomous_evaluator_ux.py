#!/usr/bin/env python3
"""
è‡ªå¾‹è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  - UXæœ€å„ªå…ˆç‰ˆ

Phaseåˆ¥worktreeã‚’è‡ªå‹•è©•ä¾¡ã—ã€UXãŒæœ€ã‚‚å„ªã‚ŒãŸã‚‚ã®ã‚’é¸æŠã™ã‚‹

è©•ä¾¡è»¸ï¼ˆåˆè¨ˆ100%ï¼‰:
  - ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ï¼ˆUXï¼‰: 35%
  - æ©Ÿèƒ½å®Œæˆåº¦: 20%
  - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: 15%
  - ãƒ†ã‚¹ãƒˆå“è³ª: 15%
  - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: 10%
  - ä¿å®ˆæ€§: 5%
"""

import json
import subprocess
import re
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from html.parser import HTMLParser
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class UXEvaluationCriteria:
    """UXé‡è¦–ã®è©•ä¾¡åŸºæº–"""
    user_experience: float = 0.35   # UXï¼ˆæœ€å„ªå…ˆï¼‰
    feature_completeness: float = 0.20  # æ©Ÿèƒ½å®Œæˆåº¦
    performance: float = 0.15       # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    test_quality: float = 0.15      # ãƒ†ã‚¹ãƒˆå“è³ª
    security: float = 0.10          # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
    maintainability: float = 0.05   # ä¿å®ˆæ€§


@dataclass
class WorktreeScore:
    """worktreeã®è©•ä¾¡ã‚¹ã‚³ã‚¢"""
    worktree_path: str
    total_score: float
    ux_score: float
    feature_score: float
    performance_score: float
    test_score: float
    security_score: float
    maintainability_score: float
    details: Dict
    ux_breakdown: Dict


class HTMLAnalyzer(HTMLParser):
    """HTMLæ§‹é€ ã‚’è§£æã—ã¦UXè©•ä¾¡"""

    def __init__(self):
        super().__init__()
        self.has_nav = False
        self.has_search = False
        self.has_breadcrumb = False
        self.aria_labels = 0
        self.interactive_elements = 0
        self.tabindex_count = 0
        self.forms = 0
        self.buttons = 0
        self.links = 0

    def handle_starttag(self, tag, attrs):
        attrs_dict = dict(attrs)

        # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
        if tag == 'nav':
            self.has_nav = True

        # æ¤œç´¢
        if tag == 'input' and attrs_dict.get('type') == 'search':
            self.has_search = True

        # ãƒ‘ãƒ³ããš
        if 'class' in attrs_dict and 'breadcrumb' in attrs_dict['class']:
            self.has_breadcrumb = True

        # ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£
        if 'aria-label' in attrs_dict or 'aria-labelledby' in attrs_dict:
            self.aria_labels += 1

        if 'tabindex' in attrs_dict:
            self.tabindex_count += 1

        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è¦ç´ 
        if tag in ['button', 'a', 'input', 'select', 'textarea']:
            self.interactive_elements += 1

        if tag == 'form':
            self.forms += 1
        if tag == 'button':
            self.buttons += 1
        if tag == 'a':
            self.links += 1


class UXAutonomousEvaluator:
    """UXé‡è¦–ã®è‡ªå¾‹è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, project_path: Path):
        self.project_path = Path(project_path)
        self.worktrees_dir = self.project_path / "worktrees"

    def evaluate_user_experience(self, worktree_path: Path) -> Tuple[float, Dict]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ï¼ˆUXï¼‰ã‚’ç·åˆè©•ä¾¡ï¼ˆ35ç‚¹æº€ç‚¹ï¼‰

        Returns:
            tuple: (UXã‚¹ã‚³ã‚¢, è©³ç´°å†…è¨³)
        """
        logger.info("  ğŸ¨ Evaluating User Experience...")

        ux_score = 0
        breakdown = {}

        # 1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹UXï¼ˆ10ç‚¹ï¼‰
        perf_ux = self._evaluate_performance_ux(worktree_path)
        ux_score += perf_ux
        breakdown['performance_ux'] = perf_ux

        # 2. ç›´æ„Ÿæ€§ãƒ»ä½¿ã„ã‚„ã™ã•ï¼ˆ10ç‚¹ï¼‰
        usability = self._evaluate_usability(worktree_path)
        ux_score += usability
        breakdown['usability'] = usability

        # 3. ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ï¼ˆ8ç‚¹ï¼‰
        accessibility = self._evaluate_accessibility(worktree_path)
        ux_score += accessibility
        breakdown['accessibility'] = accessibility

        # 4. ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œï¼ˆ7ç‚¹ï¼‰
        responsive = self._evaluate_responsive_design(worktree_path)
        ux_score += responsive
        breakdown['responsive'] = responsive

        # 100ç‚¹æº€ç‚¹ã«æ­£è¦åŒ–
        ux_score_normalized = (ux_score / 35) * 100

        logger.info(f"    âœ… UX Score: {ux_score_normalized:.1f}/100 (raw: {ux_score:.1f}/35)")
        logger.info(f"       Performance UX: {perf_ux:.1f}/10")
        logger.info(f"       Usability: {usability:.1f}/10")
        logger.info(f"       Accessibility: {accessibility:.1f}/8")
        logger.info(f"       Responsive: {responsive:.1f}/7")

        return ux_score_normalized, breakdown

    def _evaluate_performance_ux(self, worktree_path: Path) -> float:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹UXè©•ä¾¡ï¼ˆ10ç‚¹æº€ç‚¹ï¼‰"""
        score = 0

        # package.jsonã§ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ç¢ºèª
        package_json = worktree_path / "package.json"
        if package_json.exists():
            with open(package_json) as f:
                pkg_data = json.load(f)
                dependencies = pkg_data.get('dependencies', {})

                # é«˜é€Ÿãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ãƒœãƒ¼ãƒŠã‚¹
                if 'next' in dependencies:
                    score += 3  # Next.jsï¼ˆApp Routerã€SSRå¯¾å¿œï¼‰
                elif 'vite' in pkg_data.get('devDependencies', {}):
                    score += 2  # Viteï¼ˆé«˜é€Ÿãƒ“ãƒ«ãƒ‰ï¼‰

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
                if 'react-lazy-load' in dependencies or 'react-lazyload' in dependencies:
                    score += 1
                if '@vercel/analytics' in dependencies:
                    score += 1

        # HTMLã§ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¡¨ç¤ºç¢ºèª
        html_files = list(worktree_path.rglob("*.html"))
        for html_file in html_files[:3]:  # æœ€åˆã®3ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ãƒã‚§ãƒƒã‚¯
            try:
                with open(html_file, encoding='utf-8') as f:
                    content = f.read().lower()
                    if 'loading' in content or 'spinner' in content:
                        score += 1
                        break
            except:
                pass

        # JSã§Optimistic UIç¢ºèª
        js_files = list(worktree_path.rglob("*.js")) + list(worktree_path.rglob("*.jsx"))
        for js_file in js_files[:5]:
            try:
                with open(js_file, encoding='utf-8') as f:
                    content = f.read()
                    if 'optimistic' in content.lower() or 'useMutation' in content:
                        score += 2
                        break
            except:
                pass

        return min(score, 10)

    def _evaluate_usability(self, worktree_path: Path) -> float:
        """ä½¿ã„ã‚„ã™ã•è©•ä¾¡ï¼ˆ10ç‚¹æº€ç‚¹ï¼‰"""
        score = 0

        html_files = list(worktree_path.rglob("*.html"))

        if not html_files:
            return 5.0  # HTMLãŒãªã„å ´åˆï¼ˆCLIç­‰ï¼‰ã¯ä¸­é–“ã‚¹ã‚³ã‚¢

        for html_file in html_files[:5]:  # æœ€å¤§5ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
            try:
                with open(html_file, encoding='utf-8') as f:
                    content = f.read()

                    analyzer = HTMLAnalyzer()
                    analyzer.feed(content)

                    # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
                    if analyzer.has_nav:
                        score += 2
                    if analyzer.has_search:
                        score += 1
                    if analyzer.has_breadcrumb:
                        score += 1

                    # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è¦ç´ ã®å……å®Ÿåº¦
                    if analyzer.buttons > 3:
                        score += 1
                    if analyzer.forms > 0:
                        score += 1

                    # æœ€åˆã®HTMLã§è©•ä¾¡å®Œäº†
                    break

            except Exception as e:
                logger.warning(f"      âš ï¸ Error analyzing {html_file.name}: {e}")

        # JSã§ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç¢ºèª
        js_files = list(worktree_path.rglob("*.js")) + list(worktree_path.rglob("*.jsx"))
        for js_file in js_files[:5]:
            try:
                with open(js_file, encoding='utf-8') as f:
                    content = f.read()

                    # try-catch
                    if 'try {' in content and 'catch' in content:
                        score += 1

                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªã‚¨ãƒ©ãƒ¼è¡¨ç¤º
                    if any(word in content for word in ['toast', 'notification', 'alert', 'snackbar']):
                        score += 2
                        break

            except:
                pass

        return min(score, 10)

    def _evaluate_accessibility(self, worktree_path: Path) -> float:
        """ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£è©•ä¾¡ï¼ˆ8ç‚¹æº€ç‚¹ï¼‰"""
        score = 0

        html_files = list(worktree_path.rglob("*.html"))

        if not html_files:
            return 4.0  # HTMLãŒãªã„å ´åˆã¯ä¸­é–“ã‚¹ã‚³ã‚¢

        for html_file in html_files[:5]:
            try:
                with open(html_file, encoding='utf-8') as f:
                    content = f.read()

                    analyzer = HTMLAnalyzer()
                    analyzer.feed(content)

                    # ARIAå±æ€§
                    if analyzer.aria_labels > 5:
                        score += 3
                    elif analyzer.aria_labels > 0:
                        score += 1

                    # ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
                    if analyzer.tabindex_count > analyzer.interactive_elements * 0.3:
                        score += 3
                    elif analyzer.tabindex_count > 0:
                        score += 1

                    # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯HTML
                    if '<main>' in content or '<article>' in content or '<section>' in content:
                        score += 2

                    break

            except Exception as e:
                logger.warning(f"      âš ï¸ Error analyzing accessibility: {e}")

        return min(score, 8)

    def _evaluate_responsive_design(self, worktree_path: Path) -> float:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œè©•ä¾¡ï¼ˆ7ç‚¹æº€ç‚¹ï¼‰"""
        score = 0

        # CSSã§ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¯ã‚¨ãƒªç¢ºèª
        css_files = list(worktree_path.rglob("*.css"))
        for css_file in css_files[:5]:
            try:
                with open(css_file, encoding='utf-8') as f:
                    content = f.read()

                    # ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¯ã‚¨ãƒªã®æ•°
                    media_queries = content.count('@media')
                    if media_queries >= 3:
                        score += 4
                    elif media_queries > 0:
                        score += 2

                    # Flexbox/Grid
                    if 'display: flex' in content or 'display: grid' in content:
                        score += 2

                    break

            except:
                pass

        # HTMLã§viewport metaç¢ºèª
        html_files = list(worktree_path.rglob("*.html"))
        for html_file in html_files[:1]:
            try:
                with open(html_file, encoding='utf-8') as f:
                    content = f.read()
                    if 'viewport' in content and 'width=device-width' in content:
                        score += 1
                        break
            except:
                pass

        return min(score, 7)

    def evaluate_feature_completeness(self, worktree_path: Path) -> float:
        """æ©Ÿèƒ½å®Œæˆåº¦è©•ä¾¡ï¼ˆ0-100ï¼‰"""
        logger.info("  âœ¨ Evaluating Feature Completeness...")

        score = 70.0  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢

        # REQUIREMENTS.mdãŒã‚ã‚Œã°ã€å®Ÿè£…ç‡ã‚’ç¢ºèª
        requirements_file = worktree_path / "REQUIREMENTS.md"
        if requirements_file.exists():
            try:
                with open(requirements_file, encoding='utf-8') as f:
                    content = f.read()

                    # ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã®å®Ÿè£…ç‡
                    total_features = content.count('- [ ]') + content.count('- [x]')
                    completed_features = content.count('- [x]')

                    if total_features > 0:
                        completion_rate = (completed_features / total_features) * 100
                        score = completion_rate
                        logger.info(f"    âœ… Feature completion: {completion_rate:.1f}% ({completed_features}/{total_features})")
                    else:
                        logger.info("    âš ï¸ No feature checklist found")

            except Exception as e:
                logger.warning(f"    âš ï¸ Error reading REQUIREMENTS.md: {e}")
        else:
            # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã§æ¨å®š
            src_files = list((worktree_path / "src").rglob("*")) if (worktree_path / "src").exists() else []
            if len(src_files) > 10:
                score = 85.0
            elif len(src_files) > 5:
                score = 75.0

        logger.info(f"    âœ… Feature Score: {score:.1f}/100")
        return score

    def evaluate_performance(self, worktree_path: Path) -> float:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ï¼ˆ0-100ï¼‰"""
        logger.info("  âš¡ Evaluating Performance...")

        try:
            benchmark_file = worktree_path / "benchmark-results.json"

            if benchmark_file.exists():
                with open(benchmark_file) as f:
                    bench_data = json.load(f)
                    avg_response_time = bench_data.get('avg_response_time_ms', 1000)

                    if avg_response_time <= 100:
                        score = 100
                    elif avg_response_time >= 1000:
                        score = 0
                    else:
                        score = 100 - ((avg_response_time - 100) / 9)

                    logger.info(f"    âœ… Performance: {score:.1f}/100 (avg: {avg_response_time}ms)")
                    return score
            else:
                logger.info("    âš ï¸ No benchmark results, using default score")
                return 75.0

        except Exception as e:
            logger.warning(f"    âš ï¸ Performance evaluation failed: {e}")
            return 75.0

    def evaluate_test_quality(self, worktree_path: Path) -> float:
        """ãƒ†ã‚¹ãƒˆå“è³ªè©•ä¾¡ï¼ˆ0-100ï¼‰"""
        logger.info("  ğŸ§ª Evaluating Test Quality...")

        try:
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            result = subprocess.run(
                ["npm", "test", "--", "--json", "--passWithNoTests"],
                cwd=worktree_path,
                capture_output=True,
                text=True,
                timeout=300
            )

            if result.returncode == 0 or "passWithNoTests" in result.stdout:
                try:
                    test_data = json.loads(result.stdout)
                    total = test_data.get('numTotalTests', 0)
                    passed = test_data.get('numPassedTests', 0)

                    if total > 0:
                        pass_rate = (passed / total) * 100
                        logger.info(f"    âœ… Test Quality: {pass_rate:.1f}% ({passed}/{total})")
                        return pass_rate
                    else:
                        logger.info("    âš ï¸ No tests found")
                        return 50.0
                except json.JSONDecodeError:
                    logger.info("    âš ï¸ Test output parsing failed")
                    return 70.0
            else:
                logger.warning(f"    âš ï¸ Tests failed")
                return 0.0

        except subprocess.TimeoutExpired:
            logger.warning("    âš ï¸ Test execution timeout")
            return 50.0
        except Exception as e:
            logger.warning(f"    âš ï¸ Test evaluation failed: {e}")
            return 70.0

    def evaluate_security(self, worktree_path: Path) -> float:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è©•ä¾¡ï¼ˆ0-100ï¼‰"""
        logger.info("  ğŸ” Evaluating Security...")

        try:
            result = subprocess.run(
                ["npm", "audit", "--json"],
                cwd=worktree_path,
                capture_output=True,
                text=True,
                timeout=60
            )

            if result.stdout:
                audit_data = json.loads(result.stdout)
                vulnerabilities = audit_data.get('metadata', {}).get('vulnerabilities', {})

                critical = vulnerabilities.get('critical', 0)
                high = vulnerabilities.get('high', 0)
                moderate = vulnerabilities.get('moderate', 0)
                low = vulnerabilities.get('low', 0)

                score = 100 - (critical * 20 + high * 10 + moderate * 5 + low * 2)
                score = max(0, score)

                logger.info(f"    âœ… Security: {score:.1f}/100 (C:{critical}, H:{high}, M:{moderate}, L:{low})")
                return score
            else:
                logger.info("    âš ï¸ No npm audit data")
                return 85.0

        except Exception as e:
            logger.warning(f"    âš ï¸ Security evaluation failed: {e}")
            return 85.0

    def evaluate_maintainability(self, worktree_path: Path) -> float:
        """ä¿å®ˆæ€§è©•ä¾¡ï¼ˆ0-100ï¼‰"""
        logger.info("  ğŸ”§ Evaluating Maintainability...")

        try:
            src_dir = worktree_path / "src"
            if not src_dir.exists():
                return 70.0

            total_lines = 0
            for file in src_dir.rglob("*.js"):
                with open(file, encoding='utf-8') as f:
                    total_lines += len(f.readlines())
            for file in src_dir.rglob("*.ts"):
                with open(file, encoding='utf-8') as f:
                    total_lines += len(f.readlines())

            # 1000è¡Œä»¥ä¸‹ãªã‚‰æº€ç‚¹ã€5000è¡Œä»¥ä¸Šãªã‚‰0ç‚¹
            if total_lines <= 1000:
                score = 100
            elif total_lines >= 5000:
                score = 30
            else:
                score = 100 - ((total_lines - 1000) / 40)

            logger.info(f"    âœ… Maintainability: {score:.1f}/100 (lines: {total_lines})")
            return max(30, score)

        except Exception as e:
            logger.warning(f"    âš ï¸ Maintainability evaluation failed: {e}")
            return 70.0

    def evaluate_worktree(
        self,
        worktree_path: Path,
        criteria: UXEvaluationCriteria
    ) -> WorktreeScore:
        """worktreeã‚’ç·åˆè©•ä¾¡ï¼ˆUXé‡è¦–ï¼‰"""

        logger.info(f"\nğŸ“Š Evaluating: {worktree_path.name}")
        logger.info("=" * 60)

        # å„é …ç›®ã‚’è©•ä¾¡
        ux_score, ux_breakdown = self.evaluate_user_experience(worktree_path)
        feature_score = self.evaluate_feature_completeness(worktree_path)
        perf_score = self.evaluate_performance(worktree_path)
        test_score = self.evaluate_test_quality(worktree_path)
        security_score = self.evaluate_security(worktree_path)
        maintainability_score = self.evaluate_maintainability(worktree_path)

        # åŠ é‡å¹³å‡ã§ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        total_score = (
            ux_score * criteria.user_experience +
            feature_score * criteria.feature_completeness +
            perf_score * criteria.performance +
            test_score * criteria.test_quality +
            security_score * criteria.security +
            maintainability_score * criteria.maintainability
        )

        result = WorktreeScore(
            worktree_path=str(worktree_path),
            total_score=total_score,
            ux_score=ux_score,
            feature_score=feature_score,
            performance_score=perf_score,
            test_score=test_score,
            security_score=security_score,
            maintainability_score=maintainability_score,
            details={
                "user_experience": f"{ux_score:.1f}",
                "feature_completeness": f"{feature_score:.1f}",
                "performance": f"{perf_score:.1f}",
                "test_quality": f"{test_score:.1f}",
                "security": f"{security_score:.1f}",
                "maintainability": f"{maintainability_score:.1f}"
            },
            ux_breakdown=ux_breakdown
        )

        logger.info(f"\nâœ… Total Score: {total_score:.1f}/100")
        logger.info(f"   UX (35%): {ux_score:.1f} Ã— 0.35 = {ux_score * 0.35:.1f}")
        logger.info(f"   Feature (20%): {feature_score:.1f} Ã— 0.20 = {feature_score * 0.20:.1f}")
        logger.info(f"   Performance (15%): {perf_score:.1f} Ã— 0.15 = {perf_score * 0.15:.1f}")
        logger.info(f"   Test Quality (15%): {test_score:.1f} Ã— 0.15 = {test_score * 0.15:.1f}")
        logger.info(f"   Security (10%): {security_score:.1f} Ã— 0.10 = {security_score * 0.10:.1f}")
        logger.info(f"   Maintainability (5%): {maintainability_score:.1f} Ã— 0.05 = {maintainability_score * 0.05:.1f}")
        logger.info("=" * 60)

        return result

    def select_best_worktree(
        self,
        worktree_names: List[str],
        criteria: Optional[UXEvaluationCriteria] = None
    ) -> Tuple[str, WorktreeScore]:
        """è¤‡æ•°ã®worktreeã‹ã‚‰æœ€è‰¯ã‚’è‡ªå‹•é¸æŠï¼ˆUXé‡è¦–ï¼‰"""

        if criteria is None:
            criteria = UXEvaluationCriteria()

        results = {}

        logger.info("\nğŸš€ Starting UX-focused autonomous evaluation...")
        logger.info(f"ğŸ“‹ Evaluating {len(worktree_names)} worktrees")
        logger.info("\nğŸ“Š Evaluation Criteria:")
        logger.info(f"   User Experience (UX): {criteria.user_experience * 100:.0f}%")
        logger.info(f"   Feature Completeness: {criteria.feature_completeness * 100:.0f}%")
        logger.info(f"   Performance: {criteria.performance * 100:.0f}%")
        logger.info(f"   Test Quality: {criteria.test_quality * 100:.0f}%")
        logger.info(f"   Security: {criteria.security * 100:.0f}%")
        logger.info(f"   Maintainability: {criteria.maintainability * 100:.0f}%")

        for wt_name in worktree_names:
            wt_path = self.worktrees_dir / wt_name
            if wt_path.exists():
                score_result = self.evaluate_worktree(wt_path, criteria)
                results[wt_name] = score_result
            else:
                logger.warning(f"âš ï¸ Worktree not found: {wt_name}")

        if not results:
            raise ValueError("No valid worktrees found for evaluation")

        # æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’é¸æŠ
        best_name = max(results, key=lambda k: results[k].total_score)
        best_score = results[best_name]

        logger.info("\n" + "=" * 60)
        logger.info("ğŸ† EVALUATION RESULTS (UX-Focused)")
        logger.info("=" * 60)

        for name, score in sorted(results.items(), key=lambda x: x[1].total_score, reverse=True):
            logger.info(f"\n{name}:")
            logger.info(f"  Total: {score.total_score:.1f}/100")
            logger.info(f"  UX: {score.ux_score:.1f}, Feature: {score.feature_score:.1f}, Perf: {score.performance_score:.1f}")

        logger.info("\n" + "=" * 60)
        logger.info("âœ… SELECTED: " + best_name)
        logger.info(f"   Total Score: {best_score.total_score:.1f}/100")
        logger.info(f"   UX Score: {best_score.ux_score:.1f}/100 (35% weight)")
        logger.info("=" * 60)

        # çµæœã‚’JSONã§ä¿å­˜
        report_path = self.project_path / "EVALUATION_REPORT_UX.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump({
                "selected": best_name,
                "evaluation_type": "UX-Focused",
                "results": {
                    name: {
                        "total_score": score.total_score,
                        "scores": score.details,
                        "ux_breakdown": score.ux_breakdown
                    }
                    for name, score in results.items()
                },
                "criteria": {
                    "user_experience": criteria.user_experience,
                    "feature_completeness": criteria.feature_completeness,
                    "performance": criteria.performance,
                    "test_quality": criteria.test_quality,
                    "security": criteria.security,
                    "maintainability": criteria.maintainability
                }
            }, f, indent=2, ensure_ascii=False)

        logger.info(f"\nğŸ“„ UX Evaluation report saved: {report_path}")

        return best_name, best_score

    def merge_to_main_and_sync(self, selected_worktree: str, phase: str = None, skip_file_check: bool = False) -> bool:
        """é¸æŠã•ã‚ŒãŸworktreeã‚’mainã«ãƒãƒ¼ã‚¸ã—ã€ä»–ã®worktreeã«åŒæœŸ

        Args:
            selected_worktree: é¸æŠã•ã‚ŒãŸworktreeåï¼ˆä¾‹: "phase2-impl-prototype-a"ï¼‰
            phase: ãƒ•ã‚§ãƒ¼ã‚ºåï¼ˆä¾‹: "phase2"ï¼‰- è‡ªå‹•åˆ¤å®šã‚‚å¯èƒ½
            skip_file_check: é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹

        Returns:
            bool: æˆåŠŸã—ãŸã‚‰True
        """
        try:
            # ãƒ•ã‚§ãƒ¼ã‚ºã‚’è‡ªå‹•åˆ¤å®š
            if phase is None:
                if 'phase1' in selected_worktree:
                    phase = 'phase1'
                elif 'phase2' in selected_worktree:
                    phase = 'phase2'
                elif 'phase4' in selected_worktree:
                    phase = 'phase4'

            logger.info("\n" + "=" * 60)
            logger.info(f"ğŸ”„ Merging {selected_worktree} to main...")
            logger.info("=" * 60)

            # ãƒ–ãƒ©ãƒ³ãƒåã‚’æ¨å®šï¼ˆworktreeåã‹ã‚‰phaseN-ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»ï¼‰
            branch_name = selected_worktree
            for prefix in ['phase1-', 'phase2-', 'phase3-', 'phase4-', 'phase5-']:
                branch_name = branch_name.replace(prefix, 'phase/')

            # mainã«ãƒãƒ¼ã‚¸ï¼ˆM4 Macå¯¾å¿œï¼‰
            git_cmd = '/usr/bin/git' if os.path.exists('/usr/bin/git') else 'git'
            subprocess.run(
                [git_cmd, 'merge', '--no-edit', branch_name],
                cwd=self.project_path,
                check=True
            )
            logger.info(f"âœ… Merged {branch_name} to main")

            # Phaseåˆ¥ã®é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªï¼ˆskip_file_check=Trueã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            if not skip_file_check:
                # Phaseåˆ¥ã«ç¢ºèªã™ã¹ããƒ•ã‚¡ã‚¤ãƒ«ã‚’å®šç¾©
                phase_required_files = {
                    'phase1': {
                        'required': ['REQUIREMENTS.md', 'SPEC.md'],
                        'optional': ['IMAGE_PROMPTS.json', 'AUDIO_PROMPTS.json', 'TECH_STACK.md', 'WBS.json']
                    },
                    'phase2': {
                        'required': ['src/', 'index.html'],
                        'optional': ['tests/', 'assets/']
                    },
                    'phase4': {
                        'required': [],
                        'optional': ['benchmark-results.json', 'coverage/']
                    }
                }

                if phase in phase_required_files:
                    config = phase_required_files[phase]
                    missing_required = []
                    missing_optional = []

                    for file in config.get('required', []):
                        file_path = self.project_path / file
                        if file_path.exists():
                            logger.info(f"  âœ… {file} - å­˜åœ¨ç¢ºèªï¼ˆå¿…é ˆï¼‰")
                        else:
                            missing_required.append(file)
                            logger.error(f"  âŒ {file} - å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

                    for file in config.get('optional', []):
                        file_path = self.project_path / file
                        if file_path.exists():
                            logger.info(f"  âœ… {file} - å­˜åœ¨ç¢ºèªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
                        else:
                            missing_optional.append(file)
                            logger.info(f"  â„¹ï¸  {file} - ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæœªç”Ÿæˆï¼‰")

                    if missing_required:
                        logger.error(f"\nâŒ å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {', '.join(missing_required)}")
                        logger.error("   â†’ ã“ã®Phaseã®æˆæœç‰©ãŒä¸å®Œå…¨ã§ã™ã€‚å†å®Ÿè¡Œã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")

                    if missing_optional:
                        logger.info(f"\nâ„¹ï¸  ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå¾Œç¶šPhaseã§ç”Ÿæˆäºˆå®šï¼‰: {', '.join(missing_optional)}")

            # ã™ã¹ã¦ã®worktreeã«åŒæœŸ
            logger.info("\nğŸ”„ Syncing to all worktrees...")
            sync_success = 0
            sync_failed = 0

            if self.worktrees_dir.exists():
                for worktree in self.worktrees_dir.iterdir():
                    if worktree.is_dir() and worktree.name != selected_worktree:
                        try:
                            # git merge main ã‚’å„worktreeã§å®Ÿè¡Œ
                            subprocess.run(
                                [git_cmd, 'merge', '--no-edit', 'main'],
                                cwd=worktree,
                                check=True,
                                capture_output=True
                            )
                            logger.info(f"  âœ… Synced to {worktree.name}")
                            sync_success += 1
                        except subprocess.CalledProcessError:
                            logger.warning(f"  âš ï¸  Failed to sync to {worktree.name}")
                            sync_failed += 1

            logger.info(f"\nâœ… Merge and sync completed! (Success: {sync_success}, Failed: {sync_failed})")
            return True

        except subprocess.CalledProcessError as e:
            logger.error(f"âŒ Merge failed: {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ Unexpected error: {e}")
            return False


def main():
    """CLI ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    import sys

    if len(sys.argv) < 2:
        print("Usage: python3 autonomous_evaluator_ux.py <project_path> [worktree1] [worktree2] ... [options]")
        print("\nOptions:")
        print("  --auto-merge         é¸æŠã•ã‚ŒãŸworktreeã‚’è‡ªå‹•ã§mainã«ãƒãƒ¼ã‚¸ã—å…¨worktreeã«åŒæœŸ")
        print("  --skip-file-check    Phaseåˆ¥ã®é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        print("  --phase=<phase>      ãƒ•ã‚§ãƒ¼ã‚ºã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼ˆphase1, phase2, phase4ï¼‰")
        print("\nExample:")
        print("  python3 autonomous_evaluator_ux.py . phase2-impl-prototype-a phase2-impl-prototype-b phase2-impl-prototype-c")
        print("  python3 autonomous_evaluator_ux.py . phase2-impl-prototype-a phase2-impl-prototype-b --auto-merge")
        sys.exit(1)

    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
    args = sys.argv[1:]
    options = [a for a in args if a.startswith('--')]
    non_options = [a for a in args if not a.startswith('--')]

    project_path = Path(non_options[0]) if non_options else Path('.')
    worktree_names = non_options[1:] if len(non_options) > 1 else []

    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³è§£æ
    auto_merge = '--auto-merge' in options
    skip_file_check = '--skip-file-check' in options
    phase = None
    for opt in options:
        if opt.startswith('--phase='):
            phase = opt.split('=')[1]

    # worktree_namesã‹ã‚‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’é™¤å¤–
    worktree_names = [w for w in worktree_names if not w.startswith('--')]

    if not worktree_names:
        # worktrees/é…ä¸‹ã®å…¨ãƒ•ã‚©ãƒ«ãƒ€ã‚’è©•ä¾¡
        worktrees_dir = project_path / "worktrees"
        if worktrees_dir.exists():
            worktree_names = [d.name for d in worktrees_dir.iterdir() if d.is_dir()]

    evaluator = UXAutonomousEvaluator(project_path)
    best_name, best_score = evaluator.select_best_worktree(worktree_names)

    print(f"\nğŸ‰ Best worktree (UX-focused): {best_name}")
    print(f"   Total score: {best_score.total_score:.1f}/100")
    print(f"   UX score: {best_score.ux_score:.1f}/100")

    # è‡ªå‹•ãƒãƒ¼ã‚¸ãƒ»åŒæœŸï¼ˆ--auto-mergeã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if auto_merge:
        print("\nğŸ”„ Auto-merge enabled - merging to main and syncing...")
        if skip_file_check:
            print("â„¹ï¸  File check skipped (--skip-file-check)")
        evaluator.merge_to_main_and_sync(best_name, phase=phase, skip_file_check=skip_file_check)


if __name__ == "__main__":
    main()
