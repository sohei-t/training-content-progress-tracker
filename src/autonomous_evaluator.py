#!/usr/bin/env python3
"""
è‡ªå¾‹è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  - Phaseåˆ¥worktreeã®è‡ªå‹•è©•ä¾¡ãƒ»é¸æŠ

è¤‡æ•°ã®worktreeã‚’è‡ªå‹•çš„ã«è©•ä¾¡ã—ã€æœ€è‰¯ã®ã‚‚ã®ã‚’é¸æŠã™ã‚‹
"""

import json
import os
import subprocess
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class EvaluationCriteria:
    """è©•ä¾¡åŸºæº–"""
    test_pass_rate: float = 0.30    # ãƒ†ã‚¹ãƒˆåˆæ ¼ç‡ã®é‡ã¿
    code_quality: float = 0.25      # ã‚³ãƒ¼ãƒ‰å“è³ªã®é‡ã¿
    performance: float = 0.20       # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®é‡ã¿
    security: float = 0.15          # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®é‡ã¿
    simplicity: float = 0.10        # ã‚·ãƒ³ãƒ—ãƒ«ã•ã®é‡ã¿


@dataclass
class WorktreeScore:
    """worktreeã®è©•ä¾¡ã‚¹ã‚³ã‚¢"""
    worktree_path: str
    total_score: float
    test_pass_rate: float
    code_quality: float
    performance: float
    security: float
    simplicity: float
    details: Dict


class AutonomousEvaluator:
    """è‡ªå¾‹è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, project_path: Path):
        self.project_path = Path(project_path)
        self.worktrees_dir = self.project_path / "worktrees"

    def evaluate_test_pass_rate(self, worktree_path: Path) -> float:
        """
        ãƒ†ã‚¹ãƒˆåˆæ ¼ç‡ã‚’è©•ä¾¡

        Returns:
            float: ã‚¹ã‚³ã‚¢ï¼ˆ0-100ï¼‰
        """
        try:
            # ãƒ†ã‚¹ãƒˆçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
            test_result_file = worktree_path / "test-results.json"

            if not test_result_file.exists():
                # ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
                result = subprocess.run(
                    ["npm", "test", "--", "--json"],
                    cwd=worktree_path,
                    capture_output=True,
                    text=True,
                    timeout=300
                )

                if result.returncode == 0:
                    test_data = json.loads(result.stdout)
                    total = test_data.get('numTotalTests', 0)
                    passed = test_data.get('numPassedTests', 0)

                    if total > 0:
                        pass_rate = (passed / total) * 100
                        logger.info(f"âœ… Test pass rate: {pass_rate:.1f}% ({passed}/{total})")
                        return pass_rate
                    else:
                        logger.warning("âš ï¸ No tests found")
                        return 50.0  # ãƒ†ã‚¹ãƒˆãŒãªã„å ´åˆã¯ä¸­é–“ã‚¹ã‚³ã‚¢
                else:
                    logger.warning(f"âš ï¸ Test execution failed: {result.stderr}")
                    return 0.0

            else:
                # æ—¢å­˜ã®çµæœã‚’èª­ã¿è¾¼ã¿
                with open(test_result_file) as f:
                    test_data = json.load(f)
                    total = test_data.get('numTotalTests', 0)
                    passed = test_data.get('numPassedTests', 0)
                    if total > 0:
                        return (passed / total) * 100
                    else:
                        return 50.0

        except Exception as e:
            logger.error(f"âŒ Error evaluating tests: {e}")
            return 0.0

    def evaluate_code_quality(self, worktree_path: Path) -> float:
        """
        ã‚³ãƒ¼ãƒ‰å“è³ªã‚’è©•ä¾¡ï¼ˆé™çš„è§£æï¼‰

        Returns:
            float: ã‚¹ã‚³ã‚¢ï¼ˆ0-100ï¼‰
        """
        try:
            # ESLintã¾ãŸã¯Pylintãªã©ã‚’å®Ÿè¡Œ
            result = subprocess.run(
                ["npx", "eslint", "src/", "--format", "json"],
                cwd=worktree_path,
                capture_output=True,
                text=True,
                timeout=60
            )

            if result.stdout:
                lint_data = json.loads(result.stdout)
                total_issues = sum(
                    len(file.get('messages', []))
                    for file in lint_data
                )

                # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’å–å¾—
                src_files = list((worktree_path / "src").rglob("*.js")) + \
                            list((worktree_path / "src").rglob("*.ts"))
                num_files = len(src_files)

                if num_files > 0:
                    issues_per_file = total_issues / num_files
                    # 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Š5å•é¡Œä»¥ä¸‹ãªã‚‰é«˜ã‚¹ã‚³ã‚¢
                    score = max(0, 100 - (issues_per_file * 10))
                    logger.info(f"âœ… Code quality score: {score:.1f} (issues: {total_issues})")
                    return score
                else:
                    return 70.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

            else:
                logger.warning("âš ï¸ Linting skipped (no output)")
                return 70.0

        except Exception as e:
            logger.warning(f"âš ï¸ Code quality check failed: {e}")
            return 70.0  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ã‚¢

    def evaluate_performance(self, worktree_path: Path) -> float:
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ï¼ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼‰

        Returns:
            float: ã‚¹ã‚³ã‚¢ï¼ˆ0-100ï¼‰
        """
        try:
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
            benchmark_file = worktree_path / "benchmark-results.json"

            if not benchmark_file.exists():
                # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
                result = subprocess.run(
                    ["npm", "run", "benchmark"],
                    cwd=worktree_path,
                    capture_output=True,
                    text=True,
                    timeout=120
                )

                if result.returncode == 0:
                    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†ç¢ºèª
                    if benchmark_file.exists():
                        with open(benchmark_file) as f:
                            bench_data = json.load(f)
                            avg_response_time = bench_data.get('avg_response_time_ms', 1000)

                            # 100msä»¥ä¸‹ãªã‚‰æº€ç‚¹ã€1000msä»¥ä¸Šãªã‚‰0ç‚¹
                            if avg_response_time <= 100:
                                score = 100
                            elif avg_response_time >= 1000:
                                score = 0
                            else:
                                score = 100 - ((avg_response_time - 100) / 9)

                            logger.info(f"âœ… Performance score: {score:.1f} (avg: {avg_response_time}ms)")
                            return score
                    else:
                        logger.warning("âš ï¸ Benchmark file not found after execution")
                        return 70.0
                else:
                    logger.warning("âš ï¸ Benchmark execution failed")
                    return 70.0

            else:
                # æ—¢å­˜ã®çµæœã‚’èª­ã¿è¾¼ã¿
                with open(benchmark_file) as f:
                    bench_data = json.load(f)
                    avg_response_time = bench_data.get('avg_response_time_ms', 1000)
                    if avg_response_time <= 100:
                        return 100
                    elif avg_response_time >= 1000:
                        return 0
                    else:
                        return 100 - ((avg_response_time - 100) / 9)

        except Exception as e:
            logger.warning(f"âš ï¸ Performance evaluation failed: {e}")
            return 70.0

    def evaluate_security(self, worktree_path: Path) -> float:
        """
        ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚’è©•ä¾¡ï¼ˆè„†å¼±æ€§ã‚¹ã‚­ãƒ£ãƒ³ï¼‰

        Returns:
            float: ã‚¹ã‚³ã‚¢ï¼ˆ0-100ï¼‰
        """
        try:
            # npm audit ã‚’å®Ÿè¡Œ
            result = subprocess.run(
                ["npm", "audit", "--json"],
                cwd=worktree_path,
                capture_output=True,
                text=True,
                timeout=60
            )

            if result.stdout:
                audit_data = json.loads(result.stdout)
                vulnerabilities = audit_data.get('metadata', {}).get('vulnerabilities', {})

                critical = vulnerabilities.get('critical', 0)
                high = vulnerabilities.get('high', 0)
                moderate = vulnerabilities.get('moderate', 0)
                low = vulnerabilities.get('low', 0)

                # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆcritical: -20, high: -10, moderate: -5, low: -2ï¼‰
                score = 100 - (critical * 20 + high * 10 + moderate * 5 + low * 2)
                score = max(0, score)

                logger.info(f"âœ… Security score: {score:.1f} (critical: {critical}, high: {high})")
                return score
            else:
                logger.warning("âš ï¸ Security audit skipped")
                return 80.0

        except Exception as e:
            logger.warning(f"âš ï¸ Security evaluation failed: {e}")
            return 80.0

    def evaluate_simplicity(self, worktree_path: Path) -> float:
        """
        ã‚·ãƒ³ãƒ—ãƒ«ã•ã‚’è©•ä¾¡ï¼ˆã‚³ãƒ¼ãƒ‰è¡Œæ•°ã€è¤‡é›‘åº¦ï¼‰

        Returns:
            float: ã‚¹ã‚³ã‚¢ï¼ˆ0-100ï¼‰
        """
        try:
            src_dir = worktree_path / "src"
            if not src_dir.exists():
                return 70.0

            # è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            total_lines = 0
            for file in src_dir.rglob("*.js"):
                with open(file) as f:
                    total_lines += len(f.readlines())
            for file in src_dir.rglob("*.ts"):
                with open(file) as f:
                    total_lines += len(f.readlines())

            # 1000è¡Œä»¥ä¸‹ãªã‚‰æº€ç‚¹ã€5000è¡Œä»¥ä¸Šãªã‚‰0ç‚¹
            if total_lines <= 1000:
                score = 100
            elif total_lines >= 5000:
                score = 0
            else:
                score = 100 - ((total_lines - 1000) / 40)

            logger.info(f"âœ… Simplicity score: {score:.1f} (lines: {total_lines})")
            return max(0, score)

        except Exception as e:
            logger.warning(f"âš ï¸ Simplicity evaluation failed: {e}")
            return 70.0

    def evaluate_worktree(
        self,
        worktree_path: Path,
        criteria: EvaluationCriteria
    ) -> WorktreeScore:
        """
        worktreeã‚’ç·åˆè©•ä¾¡

        Args:
            worktree_path: è©•ä¾¡å¯¾è±¡ã®worktreeãƒ‘ã‚¹
            criteria: è©•ä¾¡åŸºæº–

        Returns:
            WorktreeScore: è©•ä¾¡çµæœ
        """
        logger.info(f"\nğŸ“Š Evaluating: {worktree_path.name}")
        logger.info("=" * 60)

        # å„é …ç›®ã‚’è©•ä¾¡
        test_score = self.evaluate_test_pass_rate(worktree_path)
        quality_score = self.evaluate_code_quality(worktree_path)
        perf_score = self.evaluate_performance(worktree_path)
        security_score = self.evaluate_security(worktree_path)
        simplicity_score = self.evaluate_simplicity(worktree_path)

        # åŠ é‡å¹³å‡ã§ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        total_score = (
            test_score * criteria.test_pass_rate +
            quality_score * criteria.code_quality +
            perf_score * criteria.performance +
            security_score * criteria.security +
            simplicity_score * criteria.simplicity
        )

        result = WorktreeScore(
            worktree_path=str(worktree_path),
            total_score=total_score,
            test_pass_rate=test_score,
            code_quality=quality_score,
            performance=perf_score,
            security=security_score,
            simplicity=simplicity_score,
            details={
                "test_pass_rate": f"{test_score:.1f}",
                "code_quality": f"{quality_score:.1f}",
                "performance": f"{perf_score:.1f}",
                "security": f"{security_score:.1f}",
                "simplicity": f"{simplicity_score:.1f}"
            }
        )

        logger.info(f"\nâœ… Total Score: {total_score:.1f}/100")
        logger.info("=" * 60)

        return result

    def select_best_worktree(
        self,
        worktree_names: List[str],
        criteria: Optional[EvaluationCriteria] = None
    ) -> tuple[str, WorktreeScore]:
        """
        è¤‡æ•°ã®worktreeã‹ã‚‰æœ€è‰¯ã‚’è‡ªå‹•é¸æŠ

        Args:
            worktree_names: è©•ä¾¡å¯¾è±¡ã®worktreeåã®ãƒªã‚¹ãƒˆ
            criteria: è©•ä¾¡åŸºæº–ï¼ˆçœç•¥æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰

        Returns:
            tuple: (é¸æŠã•ã‚ŒãŸworktreeå, è©•ä¾¡çµæœ)
        """
        if criteria is None:
            criteria = EvaluationCriteria()

        results = {}

        logger.info("\nğŸš€ Starting autonomous evaluation...")
        logger.info(f"ğŸ“‹ Evaluating {len(worktree_names)} worktrees")

        for wt_name in worktree_names:
            wt_path = self.worktrees_dir / wt_name
            if wt_path.exists():
                score_result = self.evaluate_worktree(wt_path, criteria)
                results[wt_name] = score_result
            else:
                logger.warning(f"âš ï¸ Worktree not found: {wt_name}")

        if not results:
            raise ValueError("No valid worktrees found for evaluation")

        # æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’é¸æŠ
        best_name = max(results, key=lambda k: results[k].total_score)
        best_score = results[best_name]

        logger.info("\n" + "=" * 60)
        logger.info("ğŸ† EVALUATION RESULTS")
        logger.info("=" * 60)

        for name, score in sorted(results.items(), key=lambda x: x[1].total_score, reverse=True):
            logger.info(f"{name}: {score.total_score:.1f}/100")

        logger.info("\nâœ… SELECTED: " + best_name)
        logger.info(f"   Score: {best_score.total_score:.1f}/100")
        logger.info("=" * 60)

        # çµæœã‚’JSONã§ä¿å­˜
        report_path = self.project_path / "EVALUATION_REPORT.json"
        with open(report_path, 'w') as f:
            json.dump({
                "selected": best_name,
                "results": {
                    name: {
                        "total_score": score.total_score,
                        "details": score.details
                    }
                    for name, score in results.items()
                },
                "criteria": {
                    "test_pass_rate": criteria.test_pass_rate,
                    "code_quality": criteria.code_quality,
                    "performance": criteria.performance,
                    "security": criteria.security,
                    "simplicity": criteria.simplicity
                }
            }, f, indent=2)

        logger.info(f"\nğŸ“„ Evaluation report saved: {report_path}")

        return best_name, best_score

    def merge_to_main_and_sync(self, selected_worktree: str, phase: str = None, skip_file_check: bool = False) -> bool:
        """é¸æŠã•ã‚ŒãŸworktreeã‚’mainã«ãƒãƒ¼ã‚¸ã—ã€ä»–ã®worktreeã«åŒæœŸ

        Args:
            selected_worktree: é¸æŠã•ã‚ŒãŸworktreeåï¼ˆä¾‹: "phase1-planning-a"ï¼‰
            phase: ãƒ•ã‚§ãƒ¼ã‚ºåï¼ˆä¾‹: "phase1"ï¼‰- è‡ªå‹•åˆ¤å®šã‚‚å¯èƒ½
            skip_file_check: é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹ï¼ˆPhase 1-Aã§ã¯ Trueï¼‰

        Returns:
            bool: æˆåŠŸã—ãŸã‚‰True
        """
        try:
            # ãƒ•ã‚§ãƒ¼ã‚ºã‚’è‡ªå‹•åˆ¤å®š
            if phase is None:
                if 'phase1' in selected_worktree:
                    phase = 'phase1'
                elif 'phase2' in selected_worktree:
                    phase = 'phase2'
                elif 'phase4' in selected_worktree:
                    phase = 'phase4'

            logger.info("\n" + "=" * 60)
            logger.info(f"ğŸ”„ Merging {selected_worktree} to main...")
            logger.info("=" * 60)

            # ãƒ–ãƒ©ãƒ³ãƒåã‚’æ¨å®šï¼ˆworktreeåã‹ã‚‰phaseN-ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»ï¼‰
            branch_name = selected_worktree
            for prefix in ['phase1-', 'phase2-', 'phase3-', 'phase4-', 'phase5-']:
                branch_name = branch_name.replace(prefix, 'phase/')

            # mainã«ãƒãƒ¼ã‚¸ï¼ˆM4 Macå¯¾å¿œï¼‰
            git_cmd = '/usr/bin/git' if os.path.exists('/usr/bin/git') else 'git'
            subprocess.run(
                [git_cmd, 'merge', '--no-edit', branch_name],
                cwd=self.project_path,
                check=True
            )
            logger.info(f"âœ… Merged {branch_name} to main")

            # Phaseåˆ¥ã®é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªï¼ˆskip_file_check=Trueã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            if not skip_file_check:
                # Phaseåˆ¥ã«ç¢ºèªã™ã¹ããƒ•ã‚¡ã‚¤ãƒ«ã‚’å®šç¾©
                phase_required_files = {
                    'phase1': {
                        'required': ['REQUIREMENTS.md', 'SPEC.md'],  # Phase 1-Aå®Œäº†æ™‚ç‚¹ã§ã®å¿…é ˆ
                        'optional': ['IMAGE_PROMPTS.json', 'AUDIO_PROMPTS.json', 'TECH_STACK.md', 'WBS.json']  # Phase 1-Bå®Œäº†æ™‚ç‚¹
                    },
                    'phase2': {
                        'required': ['src/', 'index.html'],
                        'optional': ['tests/', 'assets/']
                    },
                    'phase4': {
                        'required': [],
                        'optional': ['benchmark-results.json', 'coverage/']
                    }
                }

                if phase in phase_required_files:
                    config = phase_required_files[phase]
                    missing_required = []
                    missing_optional = []

                    for file in config.get('required', []):
                        file_path = self.project_path / file
                        if file_path.exists():
                            logger.info(f"  âœ… {file} - å­˜åœ¨ç¢ºèªï¼ˆå¿…é ˆï¼‰")
                        else:
                            missing_required.append(file)
                            logger.error(f"  âŒ {file} - å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

                    for file in config.get('optional', []):
                        file_path = self.project_path / file
                        if file_path.exists():
                            logger.info(f"  âœ… {file} - å­˜åœ¨ç¢ºèªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
                        else:
                            missing_optional.append(file)
                            logger.info(f"  â„¹ï¸  {file} - ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæœªç”Ÿæˆï¼‰")

                    if missing_required:
                        logger.error(f"\nâŒ å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {', '.join(missing_required)}")
                        logger.error("   â†’ ã“ã®Phaseã®æˆæœç‰©ãŒä¸å®Œå…¨ã§ã™ã€‚å†å®Ÿè¡Œã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")

                    if missing_optional:
                        logger.info(f"\nâ„¹ï¸  ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå¾Œç¶šPhaseã§ç”Ÿæˆäºˆå®šï¼‰: {', '.join(missing_optional)}")

            # ã™ã¹ã¦ã®worktreeã«åŒæœŸ
            logger.info("\nğŸ”„ Syncing to all worktrees...")
            sync_success = 0
            sync_failed = 0

            if self.worktrees_dir.exists():
                for worktree in self.worktrees_dir.iterdir():
                    if worktree.is_dir() and worktree.name != selected_worktree:
                        try:
                            # git merge main ã‚’å„worktreeã§å®Ÿè¡Œ
                            subprocess.run(
                                [git_cmd, 'merge', '--no-edit', 'main'],
                                cwd=worktree,
                                check=True,
                                capture_output=True
                            )
                            logger.info(f"  âœ… Synced to {worktree.name}")
                            sync_success += 1
                        except subprocess.CalledProcessError:
                            logger.warning(f"  âš ï¸  Failed to sync to {worktree.name}")
                            sync_failed += 1

            logger.info(f"\nâœ… Merge and sync completed! (Success: {sync_success}, Failed: {sync_failed})")
            return True

        except subprocess.CalledProcessError as e:
            logger.error(f"âŒ Merge failed: {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ Unexpected error: {e}")
            return False


def main():
    """CLI ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    import sys

    if len(sys.argv) < 2:
        print("Usage: python3 autonomous_evaluator.py <project_path> [worktree1] [worktree2] ... [options]")
        print("\nOptions:")
        print("  --auto-merge         é¸æŠã•ã‚ŒãŸworktreeã‚’è‡ªå‹•ã§mainã«ãƒãƒ¼ã‚¸ã—å…¨worktreeã«åŒæœŸ")
        print("  --skip-file-check    Phaseåˆ¥ã®é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆPhase 1å‰åŠç”¨ï¼‰")
        print("  --phase=<phase>      ãƒ•ã‚§ãƒ¼ã‚ºã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼ˆphase1, phase2, phase4ï¼‰")
        print("\nExample:")
        print("  python3 autonomous_evaluator.py ~/Desktop/AI-Apps/myapp-agent phase2-impl-prototype-a phase2-impl-prototype-b")
        print("  python3 autonomous_evaluator.py . phase1-planning-a phase1-planning-b --auto-merge")
        print("  python3 autonomous_evaluator.py . phase1-planning-a phase1-planning-b --auto-merge --skip-file-check")
        sys.exit(1)

    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
    args = sys.argv[1:]
    options = [a for a in args if a.startswith('--')]
    non_options = [a for a in args if not a.startswith('--')]

    project_path = Path(non_options[0]) if non_options else Path('.')
    worktree_names = non_options[1:] if len(non_options) > 1 else []

    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³è§£æ
    auto_merge = '--auto-merge' in options
    skip_file_check = '--skip-file-check' in options
    phase = None
    for opt in options:
        if opt.startswith('--phase='):
            phase = opt.split('=')[1]

    # worktree_namesã‹ã‚‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’é™¤å¤–
    worktree_names = [w for w in worktree_names if not w.startswith('--')]

    if not worktree_names:
        # worktrees/é…ä¸‹ã®å…¨ãƒ•ã‚©ãƒ«ãƒ€ã‚’è©•ä¾¡
        worktrees_dir = project_path / "worktrees"
        if worktrees_dir.exists():
            worktree_names = [d.name for d in worktrees_dir.iterdir() if d.is_dir()]

    evaluator = AutonomousEvaluator(project_path)
    best_name, best_score = evaluator.select_best_worktree(worktree_names)

    print(f"\nğŸ‰ Best worktree: {best_name}")
    print(f"   Total score: {best_score.total_score:.1f}/100")

    # è‡ªå‹•ãƒãƒ¼ã‚¸ãƒ»åŒæœŸï¼ˆ--auto-mergeã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if auto_merge:
        print("\nğŸ”„ Auto-merge enabled - merging to main and syncing...")
        if skip_file_check:
            print("â„¹ï¸  File check skipped (--skip-file-check)")
        evaluator.merge_to_main_and_sync(best_name, phase=phase, skip_file_check=skip_file_check)


if __name__ == "__main__":
    main()
